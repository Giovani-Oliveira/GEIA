{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7e7f77-212b-4cf2-b5a1-d71d80ded74a",
   "metadata": {},
   "source": [
    "# Treinando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd231ba4-999b-4064-a49f-48f715e5d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import music21 as m21\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2523bf7-766f-4905-94f7-47b15741703f",
   "metadata": {
    "tags": []
   },
   "source": [
    "m21.environment.set('musescoreDirectPNGPath', 'C:\\\\Program Files\\\\MuseScore 4\\\\bin\\\\MuseScore4.exe')\n",
    "m21.environment.set('musicxmlPath', 'C:\\\\Program Files\\\\MuseScore 4\\\\bin\\\\MuseScore4.exe')\n",
    "m21.environment.set('midiPath', 'C:\\\\Program Files\\\\MuseScore 4\\\\bin\\\\MuseScore4.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38debcc6-c720-4fd3-b347-74b9c703a064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_UNITS = 55 # Verificar sempre se tem a mesma quantidade de notas do json\n",
    "NUM_UNITS = [256]\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 120\n",
    "BATCH_SIZE = 128\n",
    "SAVE_MODEL_PATH = \"model_bach.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9c9b29-0f44-418a-a2e7-c8c3e89d7f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SINGLE_FILE_DATASET = \"file_dataset_bach\"\n",
    "MAPPING_PATH = \"mapping_bach.json\"\n",
    "SEQUENCE_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d366e4c-9dd4-4ceb-a753-615ac074a6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load(file_path):\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        song = fp.read()\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cbb80b-e42c-4a2b-92ec-d5f1d9fcb947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_songs_to_int(songs):\n",
    "    int_songs = []\n",
    "\n",
    "    # load mappings\n",
    "    with open(MAPPING_PATH, \"r\") as fp:\n",
    "        mappings = json.load(fp)\n",
    "\n",
    "    # transform songs string to list\n",
    "    songs = songs.split()\n",
    "\n",
    "    # map songs to int\n",
    "    for symbol in songs:\n",
    "        int_songs.append(mappings[symbol])\n",
    "\n",
    "    return int_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2eaaad-0a7d-4146-bfc9-2a4d23849d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_training_sequences(sequence_length):\n",
    "    \"\"\"Create input and output data samples for training. Each sample is a sequence.\n",
    "\n",
    "    :param sequence_length (int): Length of each sequence. With a quantisation at 16th notes, 64 notes equates to 4 bars\n",
    "\n",
    "    :return inputs (ndarray): Training inputs\n",
    "    :return targets (ndarray): Training targets\n",
    "    \"\"\"\n",
    "\n",
    "    # load songs and map them to int\n",
    "    songs = load(SINGLE_FILE_DATASET)\n",
    "    int_songs = convert_songs_to_int(songs)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    # generate the training sequences\n",
    "    num_sequences = len(int_songs) - sequence_length\n",
    "    for i in range(num_sequences):\n",
    "        inputs.append(int_songs[i:i+sequence_length])\n",
    "        targets.append(int_songs[i+sequence_length])\n",
    "\n",
    "    # one-hot encode the sequences\n",
    "    vocabulary_size = len(set(int_songs))\n",
    "    # inputs size: (# of sequences, sequence length, vocabulary size)\n",
    "    inputs = keras.utils.to_categorical(inputs, num_classes=vocabulary_size)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefa9d72-88d4-4761-9888-581d83884530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(output_units, num_units, loss, learning_rate):\n",
    "    \"\"\"Builds and compiles model\n",
    "\n",
    "    :param output_units (int): Num output units\n",
    "    :param num_units (list of int): Num of units in hidden layers\n",
    "    :param loss (str): Type of loss function to use\n",
    "    :param learning_rate (float): Learning rate to apply\n",
    "\n",
    "    :return model (tf model): Where the magic happens :D\n",
    "    \"\"\"\n",
    "\n",
    "    # create the model architecture\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "    x = keras.layers.LSTM(num_units[0])(input)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    output = keras.layers.Dense(output_units, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(input, output)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4be980d-d1b2-4d60-9b33-1db9f0f1f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output_units=OUTPUT_UNITS, num_units=NUM_UNITS, loss=LOSS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"Train and save TF model.\n",
    "\n",
    "    :param output_units (int): Num output units\n",
    "    :param num_units (list of int): Num of units in hidden layers\n",
    "    :param loss (str): Type of loss function to use\n",
    "    :param learning_rate (float): Learning rate to apply\n",
    "    \"\"\"\n",
    "\n",
    "    # generate the training sequences\n",
    "    inputs, targets = generate_training_sequences(SEQUENCE_LENGTH)\n",
    "\n",
    "    # build the network\n",
    "    model = build_model(output_units, num_units, loss, learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # save the model\n",
    "    model.save(SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db92a97-4c3a-4a22-a6f5-1ae3f27a973b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 55)]        0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               319488    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 55)                14135     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333,623\n",
      "Trainable params: 333,623\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "963/963 [==============================] - 163s 168ms/step - loss: 0.8883 - accuracy: 0.8008\n",
      "Epoch 2/120\n",
      "963/963 [==============================] - 167s 174ms/step - loss: 0.5639 - accuracy: 0.8287\n",
      "Epoch 3/120\n",
      "963/963 [==============================] - 166s 172ms/step - loss: 0.5243 - accuracy: 0.8403\n",
      "Epoch 4/120\n",
      "963/963 [==============================] - 166s 173ms/step - loss: 0.5098 - accuracy: 0.8451\n",
      "Epoch 5/120\n",
      "963/963 [==============================] - 166s 172ms/step - loss: 0.4785 - accuracy: 0.8543\n",
      "Epoch 6/120\n",
      "963/963 [==============================] - 166s 172ms/step - loss: 0.4691 - accuracy: 0.8572\n",
      "Epoch 7/120\n",
      "963/963 [==============================] - 166s 172ms/step - loss: 0.4733 - accuracy: 0.8579\n",
      "Epoch 8/120\n",
      "963/963 [==============================] - 166s 172ms/step - loss: 0.4986 - accuracy: 0.8509\n",
      "Epoch 9/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.4561 - accuracy: 0.8609\n",
      "Epoch 10/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.4364 - accuracy: 0.8660\n",
      "Epoch 11/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.5763 - accuracy: 0.8462\n",
      "Epoch 12/120\n",
      "963/963 [==============================] - 166s 173ms/step - loss: 0.5005 - accuracy: 0.8526\n",
      "Epoch 13/120\n",
      "963/963 [==============================] - 166s 173ms/step - loss: 0.4590 - accuracy: 0.8626\n",
      "Epoch 14/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.4289 - accuracy: 0.8688\n",
      "Epoch 15/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.4146 - accuracy: 0.8720\n",
      "Epoch 16/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.4049 - accuracy: 0.8746\n",
      "Epoch 17/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.3977 - accuracy: 0.8766\n",
      "Epoch 18/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.3900 - accuracy: 0.8785\n",
      "Epoch 19/120\n",
      "963/963 [==============================] - 167s 173ms/step - loss: 0.3772 - accuracy: 0.8823\n",
      "Epoch 20/120\n",
      "963/963 [==============================] - 167s 174ms/step - loss: 0.3692 - accuracy: 0.8848\n",
      "Epoch 21/120\n",
      "963/963 [==============================] - 168s 174ms/step - loss: 0.3536 - accuracy: 0.8891\n",
      "Epoch 22/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.3423 - accuracy: 0.8928\n",
      "Epoch 23/120\n",
      "963/963 [==============================] - 169s 175ms/step - loss: 0.3293 - accuracy: 0.8964\n",
      "Epoch 24/120\n",
      "963/963 [==============================] - 169s 175ms/step - loss: 0.3182 - accuracy: 0.9011\n",
      "Epoch 25/120\n",
      "963/963 [==============================] - 168s 175ms/step - loss: 0.3073 - accuracy: 0.9026\n",
      "Epoch 26/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.2915 - accuracy: 0.9077\n",
      "Epoch 27/120\n",
      "963/963 [==============================] - 169s 175ms/step - loss: 0.2769 - accuracy: 0.9124\n",
      "Epoch 28/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.2643 - accuracy: 0.9162\n",
      "Epoch 29/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.2550 - accuracy: 0.9184\n",
      "Epoch 30/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.2437 - accuracy: 0.9225\n",
      "Epoch 31/120\n",
      "963/963 [==============================] - 169s 175ms/step - loss: 0.2332 - accuracy: 0.9251\n",
      "Epoch 32/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.2232 - accuracy: 0.9289\n",
      "Epoch 33/120\n",
      "963/963 [==============================] - 170s 176ms/step - loss: 0.2145 - accuracy: 0.9313\n",
      "Epoch 34/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.2043 - accuracy: 0.9337\n",
      "Epoch 35/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.1962 - accuracy: 0.9369\n",
      "Epoch 36/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.1869 - accuracy: 0.9393\n",
      "Epoch 37/120\n",
      "963/963 [==============================] - 169s 176ms/step - loss: 0.1842 - accuracy: 0.9403\n",
      "Epoch 38/120\n",
      "963/963 [==============================] - 170s 176ms/step - loss: 0.1775 - accuracy: 0.9421\n",
      "Epoch 39/120\n",
      "963/963 [==============================] - 170s 176ms/step - loss: 0.1761 - accuracy: 0.9429\n",
      "Epoch 40/120\n",
      "963/963 [==============================] - 170s 176ms/step - loss: 0.1642 - accuracy: 0.9467\n",
      "Epoch 41/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1605 - accuracy: 0.9475\n",
      "Epoch 42/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1536 - accuracy: 0.9495\n",
      "Epoch 43/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1527 - accuracy: 0.9501\n",
      "Epoch 44/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1445 - accuracy: 0.9528\n",
      "Epoch 45/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.1464 - accuracy: 0.9515\n",
      "Epoch 46/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.1391 - accuracy: 0.9544\n",
      "Epoch 47/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1317 - accuracy: 0.9566\n",
      "Epoch 48/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1289 - accuracy: 0.9578\n",
      "Epoch 49/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1298 - accuracy: 0.9567\n",
      "Epoch 50/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1247 - accuracy: 0.9586\n",
      "Epoch 51/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.1214 - accuracy: 0.9603\n",
      "Epoch 52/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1185 - accuracy: 0.9610\n",
      "Epoch 53/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1147 - accuracy: 0.9622\n",
      "Epoch 54/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.1110 - accuracy: 0.9632\n",
      "Epoch 55/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1246 - accuracy: 0.9588\n",
      "Epoch 56/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1071 - accuracy: 0.9647\n",
      "Epoch 57/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.1014 - accuracy: 0.9668\n",
      "Epoch 58/120\n",
      "963/963 [==============================] - 170s 177ms/step - loss: 0.1077 - accuracy: 0.9641\n",
      "Epoch 59/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1034 - accuracy: 0.9666\n",
      "Epoch 60/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.1023 - accuracy: 0.9662\n",
      "Epoch 61/120\n",
      "963/963 [==============================] - 171s 177ms/step - loss: 0.0980 - accuracy: 0.9675\n",
      "Epoch 62/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0974 - accuracy: 0.9681\n",
      "Epoch 63/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0948 - accuracy: 0.9682\n",
      "Epoch 64/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.1080 - accuracy: 0.9646\n",
      "Epoch 65/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0933 - accuracy: 0.9696\n",
      "Epoch 66/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.1004 - accuracy: 0.9666\n",
      "Epoch 67/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0919 - accuracy: 0.9693\n",
      "Epoch 68/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0888 - accuracy: 0.9709\n",
      "Epoch 69/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0913 - accuracy: 0.9699\n",
      "Epoch 70/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0885 - accuracy: 0.9703\n",
      "Epoch 71/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0870 - accuracy: 0.9708\n",
      "Epoch 72/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0856 - accuracy: 0.9718\n",
      "Epoch 73/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.1023 - accuracy: 0.9666\n",
      "Epoch 74/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0848 - accuracy: 0.9718\n",
      "Epoch 75/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0806 - accuracy: 0.9737\n",
      "Epoch 76/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0835 - accuracy: 0.9726\n",
      "Epoch 77/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0841 - accuracy: 0.9719\n",
      "Epoch 78/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0808 - accuracy: 0.9733\n",
      "Epoch 79/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.1155 - accuracy: 0.9629\n",
      "Epoch 80/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0885 - accuracy: 0.9705\n",
      "Epoch 81/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0753 - accuracy: 0.9753\n",
      "Epoch 82/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0755 - accuracy: 0.9751\n",
      "Epoch 83/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0778 - accuracy: 0.9740\n",
      "Epoch 84/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0826 - accuracy: 0.9725\n",
      "Epoch 85/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0771 - accuracy: 0.9749\n",
      "Epoch 86/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0759 - accuracy: 0.9748\n",
      "Epoch 87/120\n",
      "963/963 [==============================] - 175s 182ms/step - loss: 0.0735 - accuracy: 0.9761\n",
      "Epoch 88/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0746 - accuracy: 0.9761\n",
      "Epoch 89/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0768 - accuracy: 0.9746\n",
      "Epoch 90/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0752 - accuracy: 0.9752\n",
      "Epoch 91/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0743 - accuracy: 0.9756\n",
      "Epoch 92/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0721 - accuracy: 0.9764\n",
      "Epoch 93/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0720 - accuracy: 0.9758\n",
      "Epoch 94/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0716 - accuracy: 0.9762\n",
      "Epoch 95/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0760 - accuracy: 0.9751\n",
      "Epoch 96/120\n",
      "963/963 [==============================] - 172s 178ms/step - loss: 0.0773 - accuracy: 0.9746\n",
      "Epoch 97/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0705 - accuracy: 0.9765\n",
      "Epoch 98/120\n",
      "963/963 [==============================] - 171s 178ms/step - loss: 0.0733 - accuracy: 0.9755\n",
      "Epoch 99/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0685 - accuracy: 0.9773\n",
      "Epoch 100/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0663 - accuracy: 0.9784\n",
      "Epoch 101/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0690 - accuracy: 0.9771\n",
      "Epoch 102/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0704 - accuracy: 0.9766\n",
      "Epoch 103/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0674 - accuracy: 0.9779\n",
      "Epoch 104/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0669 - accuracy: 0.9782\n",
      "Epoch 105/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0736 - accuracy: 0.9757\n",
      "Epoch 106/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0682 - accuracy: 0.9776\n",
      "Epoch 107/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0667 - accuracy: 0.9777\n",
      "Epoch 108/120\n",
      "963/963 [==============================] - 175s 181ms/step - loss: 0.0653 - accuracy: 0.9784\n",
      "Epoch 109/120\n",
      "963/963 [==============================] - 173s 179ms/step - loss: 0.0638 - accuracy: 0.9787\n",
      "Epoch 110/120\n",
      "963/963 [==============================] - 172s 179ms/step - loss: 0.0662 - accuracy: 0.9783\n",
      "Epoch 111/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0662 - accuracy: 0.9782\n",
      "Epoch 112/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0673 - accuracy: 0.9777\n",
      "Epoch 113/120\n",
      "963/963 [==============================] - 173s 180ms/step - loss: 0.0643 - accuracy: 0.9792\n",
      "Epoch 114/120\n",
      "963/963 [==============================] - 178s 185ms/step - loss: 0.0627 - accuracy: 0.9793\n",
      "Epoch 115/120\n",
      "963/963 [==============================] - 163s 169ms/step - loss: 0.0651 - accuracy: 0.9785\n",
      "Epoch 116/120\n",
      "963/963 [==============================] - 147s 153ms/step - loss: 0.0628 - accuracy: 0.9790\n",
      "Epoch 117/120\n",
      "963/963 [==============================] - 147s 153ms/step - loss: 0.0621 - accuracy: 0.9793\n",
      "Epoch 118/120\n",
      "963/963 [==============================] - 147s 153ms/step - loss: 0.0627 - accuracy: 0.9790\n",
      "Epoch 119/120\n",
      "963/963 [==============================] - 148s 153ms/step - loss: 0.0644 - accuracy: 0.9788\n",
      "Epoch 120/120\n",
      "963/963 [==============================] - 149s 154ms/step - loss: 0.0614 - accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83abd7-707f-4290-8e14-7e3cca29ca2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3fba3-f049-4f06-b835-5ada9586fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
